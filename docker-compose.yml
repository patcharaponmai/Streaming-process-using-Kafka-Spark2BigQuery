version: '3'
services:
  jupyter:
    build:
      context: .
      dockerfile: Dockerfile
    image: jupyter:1.0
    container_name: jupyter-notebook-kafka-gcp-streaming-project
    volumes:
      - ./source_code:/app
    ports:
      - "8888:8888"
    depends_on:
      - postgres
    environment:
      PGHOST: postgres
      PGUSER: root
      PGPASSWORD: P@ssw0rd
      PGDATABASE: online_shopping_db_kafka
      GOOGLE_APPLICATION_CREDENTIALS: ./secrete_key/planar-beach-402009-5c3ff91bc2fa.json
    command: ["start-notebook.sh", "--NotebookApp.token=''"]

  zookeeper:
    image: confluentinc/cp-zookeeper:7.0.0
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000

  kafka:
    image: confluentinc/cp-kafka:7.0.0
    depends_on:
      - zookeeper
    ports:
      - 9092:9092
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_LISTENERS: PLAINTEXT://:9092
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
      KAFKA_DELETE_TOPIC_ENABLE: "true"
      KAFKA_LOG_DIRS: /var/lib/kafka/data

  postgres:
    image: postgres:latest
    container_name: postgres-db-kafka-gcp-streaming-project
    restart: always
    environment:
      POSTGRES_USER: "root"
      POSTGRES_PASSWORD: "P@ssw0rd"
      POSTGRES_DB: "online_shopping_db_kafka"
    ports:
      - "5432:5432"
    volumes:
      - ./init.sql:/docker-entrypoint-initdb.d/init.sql

  spark:
    image: bitnami/spark:3.1.1
    ports:
      - 4040:4040
    depends_on:
      - kafka
    environment:
      - SPARK_MASTER=local
      - SPARK_APPLICATION_ARGS=--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.1 /path/to/your/spark_script.py
    volumes:
      - /path/to/your/spark_script.py:/opt/bitnami/spark/scripts/spark_script.py

networks:
  default:
    driver: bridge
